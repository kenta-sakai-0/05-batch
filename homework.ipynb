{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.4.4\n",
      "  Using cached pyspark-3.4.4.tar.gz (311.4 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pandas==2.2.1\n",
      "  Using cached pandas-2.2.1-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Collecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kenta sakai\\projects\\data-engineering-zoomcamp\\cohorts\\2025\\05-batch\\venv\\lib\\site-packages (from pandas==2.2.1->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Collecting numpy<2,>=1.22.4\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kenta sakai\\projects\\data-engineering-zoomcamp\\cohorts\\2025\\05-batch\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.1->-r requirements.txt (line 2)) (1.17.0)\n",
      "Installing collected packages: pytz, py4j, tzdata, pyspark, numpy, pandas\n",
      "  Running setup.py install for pyspark: started\n",
      "  Running setup.py install for pyspark: finished with status 'done'\n",
      "Successfully installed numpy-1.26.4 pandas-2.2.1 py4j-0.10.9.7 pyspark-3.4.4 pytz-2025.1 tzdata-2025.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: pyspark is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['JAVA_HOME'] = r'C:\\tools\\jdk-11.0.25'\n",
    "os.environ['HADOOP_HOME'] = r'C:\\tools\\hadoop-3.2.0'\n",
    "os.environ['PATH'] = f\"{os.environ['HADOOP_HOME']}\\\\bin;{os.environ['PATH']}\"  # Add Hadoop to PATH\n",
    "os.environ['SPARK_HOME'] = r'C:\\tools\\spark-3.4.4-bin-hadoop3'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 16 61.3M   16  9.9M    0     0  9525k      0  0:00:06  0:00:01  0:00:05 9525k\n",
      " 34 61.3M   34 21.2M    0     0  10.2M      0  0:00:05  0:00:02  0:00:03 10.2M\n",
      " 53 61.3M   53 32.5M    0     0  10.5M      0  0:00:05  0:00:03  0:00:02 10.5M\n",
      " 71 61.3M   71 43.8M    0     0  10.7M      0  0:00:05  0:00:04  0:00:01 10.7M\n",
      " 89 61.3M   89 55.0M    0     0  10.8M      0  0:00:05  0:00:05 --:--:-- 11.0M\n",
      "100 61.3M  100 61.3M    0     0  10.8M      0  0:00:05  0:00:05 --:--:-- 11.2M\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o yellow_tripdata_2024-10.parquet https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .parquet('yellow_tripdata_2024-10.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('data/yellow/2024/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128893"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime \n",
    "from pyspark.sql.functions import to_date\n",
    "oct15 = datetime(2024,10,15)\n",
    "\n",
    "oct15_df = df.filter(to_date(df.tpep_pickup_datetime) == oct15)\n",
    "oct15_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kenta Sakai\\projects\\data-engineering-zoomcamp\\cohorts\\2025\\05-batch\\venv\\lib\\site-packages\\pyspark\\sql\\dataframe.py:330: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable(\"trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT \n",
    "          (unix_timestamp(tpep_dropoff_datetime) - unix_timestamp(tpep_pickup_datetime)) / 3600 as hours,\n",
    "          tpep_dropoff_datetime,\n",
    "          tpep_pickup_datetime\n",
    "          FROM trips\n",
    "          order by 1 desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 12331  100 12331    0     0  96278      0 --:--:-- --:--:-- --:--:-- 96335\n"
     ]
    }
   ],
   "source": [
    "!curl -o data/taxi_zone_lookup.csv https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"data/taxi_zone_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kenta Sakai\\projects\\data-engineering-zoomcamp\\cohorts\\2025\\05-batch\\venv\\lib\\site-packages\\pyspark\\sql\\dataframe.py:330: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "zones.registerTempTable(\"zones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LocationID', 'Borough', 'Zone', 'service_zone']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zones.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge',\n",
       " 'Airport_fee']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Zone|\n",
      "+--------------------+\n",
      "|Governor's Island...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    WITH least AS (\n",
    "        SELECT PULocationID, COUNT(*)\n",
    "        FROM trips t \n",
    "        GROUP BY 1 \n",
    "        ORDER BY 2 ASC\n",
    "        LIMIT 1\n",
    "    )\n",
    "    \n",
    "    SELECT Zone FROM zones WHERE LocationID in (SELECT PULocationID FROM least )\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
